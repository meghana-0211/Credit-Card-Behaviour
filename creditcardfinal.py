# -*- coding: utf-8 -*-
"""creditcardfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O88_W7bKlf0u_DIvbrDPmr7SNtxefrBT
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, confusion_matrix, classification_report
import warnings
import zipfile
import os
warnings.filterwarnings('ignore')

class CreditScoringStack:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state
        self.base_models = {
            'xgb': xgb.XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state
            ),
            'lgb': lgb.LGBMClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state
            ),
            'rf': RandomForestClassifier(
                n_estimators=200,
                max_depth=4,
                min_samples_split=50,
                min_samples_leaf=20,
                random_state=random_state
            )
        }
        self.meta_model = LogisticRegression(random_state=random_state)
        self.scaler = StandardScaler()

    def preprocess_data(self, data):
        """Preprocess the input data"""
        # Handle missing values
        data = data.fillna(0)

        # Separate features by type
        onus_cols = [col for col in data.columns if col.startswith('onus_attribute')]
        trans_cols = [col for col in data.columns if col.startswith('transaction_attribute')]
        bureau_cols = [col for col in data.columns if col.startswith('bureau') and not col.startswith('bureau_enquiry')]
        enquiry_cols = [col for col in data.columns if col.startswith('bureau_enquiry')]

        # Scale numerical features
        numerical_cols = onus_cols + trans_cols + bureau_cols + enquiry_cols
        data[numerical_cols] = self.scaler.fit_transform(data[numerical_cols])

        return data

    def generate_meta_features(self, X, y=None, is_train=True):
        """Generate meta-features using k-fold cross-validation"""
        if is_train:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

            # Generate out-of-fold predictions
            for i, (train_idx, valid_idx) in enumerate(kf.split(X)):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_valid_fold = X.iloc[valid_idx]

                # Train each base model and make predictions
                for j, (name, model) in enumerate(self.base_models.items()):
                    model.fit(X_train_fold, y_train_fold)
                    meta_features[valid_idx, j] = model.predict_proba(X_valid_fold)[:, 1]

            return meta_features
        else:
            # For test data, use full training for base models
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            for j, (name, model) in enumerate(self.base_models.items()):
                meta_features[:, j] = model.predict_proba(X)[:, 1]
            return meta_features

    def fit(self, X, y):
        """Fit the stacking ensemble"""
        # Generate meta-features
        meta_features = self.generate_meta_features(X, y, is_train=True)

        # Fit meta-model
        self.meta_model.fit(meta_features, y)

        # Retrain base models on full data
        for name, model in self.base_models.items():
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        """Generate probability predictions"""
        meta_features = self.generate_meta_features(X, is_train=False)
        return self.meta_model.predict_proba(meta_features)[:, 1]

    def predict(self, X, threshold=0.5):
        """Generate class predictions based on probability threshold"""
        probas = self.predict_proba(X)
        return (probas >= threshold).astype(int)

    def evaluate(self, X, y, threshold=0.5):
        """Evaluate model performance with multiple metrics"""
        # Get probability predictions
        y_pred_proba = self.predict_proba(X)

        # Get class predictions
        y_pred = (y_pred_proba >= threshold).astype(int)

        # Calculate confusion matrix
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()

        # Calculate various metrics
        accuracy = accuracy_score(y, y_pred)
        auc_score = roc_auc_score(y, y_pred_proba)
        avg_precision = average_precision_score(y, y_pred_proba)
        precision, recall, _ = precision_recall_curve(y, y_pred_proba)

        # Calculate additional metrics
        specificity = tn / (tn + fp)
        sensitivity = tp / (tp + fn)  # Same as recall
        f1_score = 2 * (sensitivity * precision[1]) / (sensitivity + precision[1])

        # Get detailed classification report
        class_report = classification_report(y, y_pred, output_dict=True)

        return {
            'accuracy': accuracy,
            'auc_score': auc_score,
            'avg_precision': avg_precision,
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'sensitivity': sensitivity,
            'f1_score': f1_score,
            'confusion_matrix': {
                'tn': tn,
                'fp': fp,
                'fn': fn,
                'tp': tp
            },
            'classification_report': class_report
        }

def extract_zip_file(zip_path, extract_path):
    """Extract zip file to specified path"""
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    # Get the extracted CSV file name
    extracted_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]
    if not extracted_files:
        raise ValueError("No CSV file found in the extracted contents")
    return os.path.join(extract_path, extracted_files[0])

def print_metrics(metrics):
    """Print evaluation metrics in a formatted way"""
    print("\nModel Performance Metrics:")
    print("=" * 50)
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"AUC Score: {metrics['auc_score']:.4f}")
    print(f"Average Precision: {metrics['avg_precision']:.4f}")
    print(f"F1 Score: {metrics['f1_score']:.4f}")
    print(f"Sensitivity (Recall): {metrics['sensitivity']:.4f}")
    print(f"Specificity: {metrics['specificity']:.4f}")

    print("\nConfusion Matrix:")
    print("-" * 50)
    print(f"True Negatives: {metrics['confusion_matrix']['tn']}")
    print(f"False Positives: {metrics['confusion_matrix']['fp']}")
    print(f"False Negatives: {metrics['confusion_matrix']['fn']}")
    print(f"True Positives: {metrics['confusion_matrix']['tp']}")

    print("\nDetailed Classification Report:")
    print("-" * 50)
    for label in ['0', '1']:
        print(f"\nClass {label}:")
        print(f"Precision: {metrics['classification_report'][label]['precision']:.4f}")
        print(f"Recall: {metrics['classification_report'][label]['recall']:.4f}")
        print(f"F1-score: {metrics['classification_report'][label]['f1-score']:.4f}")

def main():
    # Create temporary directory for extracted files
    temp_dir = 'temp_extracted'
    os.makedirs(temp_dir, exist_ok=True)

    try:
        # Load development data
        dev_data = pd.read_csv('/content/cleaned_dev.csv')

        # Extract and load validation data from zip
        val_csv_path = extract_zip_file('/content/validation_data_to_be_shared 3.zip', temp_dir)
        val_data = pd.read_csv(val_csv_path)

        # Prepare features and target
        target = 'bad_flag'
        features = [col for col in dev_data.columns if col not in [target, 'account_number']]

        # Initialize and train model
        model = CreditScoringStack()
        X = model.preprocess_data(dev_data[features])
        y = dev_data[target]

        # Fit model
        model.fit(X, y)

        # Preprocess validation data
        X_val = model.preprocess_data(val_data[features])

        # Generate predictions
        val_predictions = model.predict_proba(X_val)

        # Create submission file
        submission = pd.DataFrame({
            'account_number': val_data['account_number'],
            'predicted_probability': val_predictions
        })

        # Save predictions
        submission.to_csv('predictions.csv', index=False)

        # If we have validation labels, evaluate performance
        if 'bad_flag' in val_data.columns:
            metrics = model.evaluate(X_val, val_data[target])
            print_metrics(metrics)

    finally:
        # Clean up temporary files
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()

model = CreditScoringStack()

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score
import warnings
warnings.filterwarnings('ignore')

class ImprovedCreditScoring:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state

        # Modified base models with better hyperparameters
        self.base_models = {
            'xgb1': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=5,
                subsample=0.85,
                colsample_bytree=0.8,
                min_child_weight=3,
                scale_pos_weight=2,  # Handle class imbalance
                random_state=random_state
            ),
            'xgb2': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.9,
                min_child_weight=4,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=6,
                min_samples_split=30,
                min_samples_leaf=15,
                class_weight='balanced',  # Handle class imbalance
                random_state=random_state
            )
        }

        # Use L2 regularization in meta model
        self.meta_model = LogisticRegression(C=0.8, class_weight='balanced', random_state=random_state)
        self.scaler = RobustScaler()  # More robust to outliers

    def preprocess_data(self, data):
        """Enhanced preprocessing with feature engineering"""
        # Handle missing values more sophisticatedly
        data = data.copy()

        # Separate features by type
        onus_cols = [col for col in data.columns if col.startswith('onus_attribute')]
        trans_cols = [col for col in data.columns if col.startswith('transaction_attribute')]
        bureau_cols = [col for col in data.columns if col.startswith('bureau') and not col.startswith('bureau_enquiry')]
        enquiry_cols = [col for col in data.columns if col.startswith('bureau_enquiry')]

        # Fill missing values based on feature type
        data[onus_cols] = data[onus_cols].fillna(data[onus_cols].median())
        data[trans_cols] = data[trans_cols].fillna(0)  # Assume missing transactions are 0
        data[bureau_cols] = data[bureau_cols].fillna(data[bureau_cols].median())
        data[enquiry_cols] = data[enquiry_cols].fillna(0)  # Assume missing enquiries are 0

        # Feature engineering
        # Transaction ratios
        for col in trans_cols:
            if 'value' in col:
                corresponding_count = col.replace('value', 'count')
                if corresponding_count in trans_cols:
                    data[f'{col}_per_transaction'] = data[col] / (data[corresponding_count] + 1)

        # Bureau ratios
        total_accounts = data[[col for col in bureau_cols if 'count' in col]].sum(axis=1)
        for col in bureau_cols:
            if 'count' in col:
                data[f'{col}_ratio'] = data[col] / (total_accounts + 1)

        # Scale features
        numerical_cols = onus_cols + trans_cols + bureau_cols + enquiry_cols
        data[numerical_cols] = self.scaler.fit_transform(data[numerical_cols])

        return data

    def generate_meta_features(self, X, y=None, is_train=True):
        """Generate meta-features using stratified k-fold"""
        if is_train:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            # Use StratifiedKFold for better handling of imbalanced data
            kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

            for i, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_valid_fold = X.iloc[valid_idx]

                for j, (name, model) in enumerate(self.base_models.items()):
                    model.fit(X_train_fold, y_train_fold)
                    meta_features[valid_idx, j] = model.predict_proba(X_valid_fold)[:, 1]

            return meta_features
        else:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            for j, (name, model) in enumerate(self.base_models.items()):
                meta_features[:, j] = model.predict_proba(X)[:, 1]
            return meta_features

    def fit(self, X, y):
        """Enhanced fitting with feature importance analysis"""
        # Generate meta-features
        meta_features = self.generate_meta_features(X, y, is_train=True)

        # Fit meta-model
        self.meta_model.fit(meta_features, y)

        # Store feature importances from XGBoost models
        self.feature_importances_ = {}
        for name, model in self.base_models.items():
            if isinstance(model, xgb.XGBClassifier):
                self.feature_importances_[name] = dict(zip(X.columns, model.feature_importances_))

        # Retrain base models on full data
        for name, model in self.base_models.items():
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        """Generate probability predictions with optional calibration"""
        meta_features = self.generate_meta_features(X, is_train=False)
        return self.meta_model.predict_proba(meta_features)[:, 1]

    def get_top_features(self, n=20):
        """Get top n most important features from XGBoost models"""
        all_importances = {}
        for name, importances in self.feature_importances_.items():
            for feature, importance in importances.items():
                if feature not in all_importances:
                    all_importances[feature] = 0
                all_importances[feature] += importance

        return dict(sorted(all_importances.items(), key=lambda x: x[1], reverse=True)[:n])

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, confusion_matrix, classification_report
import warnings
import zipfile
import os
warnings.filterwarnings('ignore')

class CreditScorer:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state

        # Define base models
        self.base_models = {
            'xgb1': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=5,
                subsample=0.85,
                colsample_bytree=0.8,
                min_child_weight=3,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'xgb2': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.9,
                min_child_weight=4,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=6,
                min_samples_split=30,
                min_samples_leaf=15,
                class_weight='balanced',
                random_state=random_state
            )
        }
        self.meta_model = LogisticRegression(C=0.8, class_weight='balanced', random_state=random_state)

    def generate_meta_features(self, X, y=None, is_train=True):
        if is_train:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

            for i, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_valid_fold = X.iloc[valid_idx]

                for j, (name, model) in enumerate(self.base_models.items()):
                    model.fit(X_train_fold, y_train_fold)
                    meta_features[valid_idx, j] = model.predict_proba(X_valid_fold)[:, 1]

            return meta_features
        else:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            for j, (name, model) in enumerate(self.base_models.items()):
                meta_features[:, j] = model.predict_proba(X)[:, 1]
            return meta_features

    def fit(self, X, y):
        meta_features = self.generate_meta_features(X, y, is_train=True)
        self.meta_model.fit(meta_features, y)

        # Retrain base models on full data
        for name, model in self.base_models.items():
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        meta_features = self.generate_meta_features(X, is_train=False)
        return self.meta_model.predict_proba(meta_features)[:, 1]

def extract_zip(zip_path, extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    extracted_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]
    if not extracted_files:
        raise ValueError("No CSV file found in the extracted contents")
    return os.path.join(extract_path, extracted_files[0])

def print_metrics(metrics):
    print("\nModel Performance Metrics:")
    print("=" * 50)
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"AUC Score: {metrics['auc_score']:.4f}")
    print(f"Average Precision: {metrics['avg_precision']:.4f}")

    print("\nConfusion Matrix:")
    print("-" * 50)
    print(f"True Negatives: {metrics['confusion_matrix']['tn']}")
    print(f"False Positives: {metrics['confusion_matrix']['fp']}")
    print(f"False Negatives: {metrics['confusion_matrix']['fn']}")
    print(f"True Positives: {metrics['confusion_matrix']['tp']}")


print("Loading development data...")
dev_data = pd.read_csv('/content/cleaned_dev.csv')

# Create temporary directory for validation data
temp_dir = 'temp_extracted'
os.makedirs(temp_dir, exist_ok=True)

try:
    # Extract and load validation data
    print("Loading validation data...")
    val_csv_path = extract_zip('/content/validation_data_to_be_shared 3.zip', temp_dir)
    val_data = pd.read_csv(val_csv_path)

    # Prepare features and target
    target = 'bad_flag'
    features = [col for col in dev_data.columns if col not in [target, 'account_number']]

    print("Training model...")
    # Initialize and train model
    model = CreditScorer()
    model.fit(dev_data[features], dev_data[target])

    print("Generating predictions...")
    # Generate predictions
    val_predictions = model.predict_proba(val_data[features])

    # Create submission file
    submission = pd.DataFrame({
        'account_number': val_data['account_number'],
        'predicted_probability': val_predictions
    })

    print("Saving predictions...")
    # Save predictions
    submission.to_csv('predictions1.csv', index=False)

    print("Done! Predictions saved to 'predictions.csv'")

finally:
    # Clean up temporary files
    import shutil
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, confusion_matrix, classification_report
import warnings
import zipfile
import os
warnings.filterwarnings('ignore')

class CreditScorer:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state

        # Define base models
        self.base_models = {
            'xgb1': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=5,
                subsample=0.85,
                colsample_bytree=0.8,
                min_child_weight=3,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'xgb2': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.9,
                min_child_weight=4,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=6,
                min_samples_split=30,
                min_samples_leaf=15,
                class_weight='balanced',
                random_state=random_state
            )
        }
        self.meta_model = LogisticRegression(C=0.8, class_weight='balanced', random_state=random_state)

    def generate_meta_features(self, X, y=None, is_train=True):
        if is_train:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

            # Store CV scores
            self.cv_scores = []

            for i, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_valid_fold = X.iloc[valid_idx]
                y_valid_fold = y.iloc[valid_idx]

                fold_preds = np.zeros(len(valid_idx))
                for j, (name, model) in enumerate(self.base_models.items()):
                    model.fit(X_train_fold, y_train_fold)
                    proba = model.predict_proba(X_valid_fold)[:, 1]
                    meta_features[valid_idx, j] = proba
                    fold_preds += proba / len(self.base_models)

                # Calculate and store fold score
                fold_score = roc_auc_score(y_valid_fold, fold_preds)
                self.cv_scores.append(fold_score)
                print(f"Fold {i+1} AUC: {fold_score:.4f}")

            print(f"\nMean CV AUC: {np.mean(self.cv_scores):.4f} (+/- {np.std(self.cv_scores):.4f})")
            return meta_features
        else:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            for j, (name, model) in enumerate(self.base_models.items()):
                meta_features[:, j] = model.predict_proba(X)[:, 1]
            return meta_features

    def fit(self, X, y):
        print("Generating meta-features and CV scores...")
        meta_features = self.generate_meta_features(X, y, is_train=True)
        self.meta_model.fit(meta_features, y)

        print("\nTraining base models on full data...")
        for name, model in self.base_models.items():
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        meta_features = self.generate_meta_features(X, is_train=False)
        return self.meta_model.predict_proba(meta_features)[:, 1]

    def evaluate(self, X, y, threshold=0.5):
        """Evaluate model performance with multiple metrics"""
        y_pred_proba = self.predict_proba(X)
        y_pred = (y_pred_proba >= threshold).astype(int)

        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()

        metrics = {
            'accuracy': accuracy_score(y, y_pred),
            'auc_score': roc_auc_score(y, y_pred_proba),
            'avg_precision': average_precision_score(y, y_pred_proba),
            'confusion_matrix': {
                'tn': tn,
                'fp': fp,
                'fn': fn,
                'tp': tp
            }
        }

        print("\nModel Performance Metrics:")
        print("=" * 50)
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"AUC Score: {metrics['auc_score']:.4f}")
        print(f"Average Precision: {metrics['avg_precision']:.4f}")

        print("\nConfusion Matrix:")
        print("-" * 50)
        print(f"True Negatives: {tn}")
        print(f"False Positives: {fp}")
        print(f"False Negatives: {fn}")
        print(f"True Positives: {tp}")

        # Calculate and print additional metrics
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        print("\nAdditional Metrics:")
        print("-" * 50)
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

        return metrics

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load development data
print("Loading development data...")
dev_data = pd.read_csv('/content/cleaned_dev.csv')

# Create temporary directory for validation data
temp_dir = 'temp_extracted'
os.makedirs(temp_dir, exist_ok=True)

try:
    # Extract and load validation data
    print("Loading validation data...")
    val_csv_path = extract_zip('/content/validation_data_to_be_shared 3.zip', temp_dir)
    val_data = pd.read_csv(val_csv_path)

    # Prepare features and target
    target = 'bad_flag'
    features = [col for col in dev_data.columns if col not in [target, 'account_number']]

    print("\nTraining model...")
    # Initialize and train model
    model = CreditScorer()
    model.fit(dev_data[features], dev_data[target])

    # Evaluate on development data
    print("\nEvaluating on development data...")
    model.evaluate(dev_data[features], dev_data[target])

    print("\nGenerating predictions for validation data...")
    # Generate predictions
    val_predictions = model.predict_proba(val_data[features])

    # Create submission file
    submission = pd.DataFrame({
        'account_number': val_data['account_number'],
        'predicted_probability': val_predictions
    })

    print("\nSaving predictions...")
    # Save predictions
    submission.to_csv('predictions2.csv', index=False)

    # If validation data has labels, evaluate on it too
    if 'bad_flag' in val_data.columns:
        print("\nEvaluating on validation data...")
        model.evaluate(val_data[features], val_data[target])

    print("\nDone! Predictions saved to 'predictions.csv'")

finally:
    # Clean up temporary files
    import shutil
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, confusion_matrix, classification_report
import warnings
import zipfile
import os
warnings.filterwarnings('ignore')

class CreditScorer:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state

        # Define base models
        self.base_models = {
            'xgb1': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=5,
                subsample=0.85,
                colsample_bytree=0.8,
                min_child_weight=3,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'xgb2': xgb.XGBClassifier(
                n_estimators=300,
                learning_rate=0.03,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.9,
                min_child_weight=4,
                scale_pos_weight=2,
                random_state=random_state
            ),
            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=6,
                min_samples_split=30,
                min_samples_leaf=15,
                class_weight='balanced',
                random_state=random_state
            )
        }
        self.meta_model = LogisticRegression(C=0.8, class_weight='balanced', random_state=random_state)

    def generate_meta_features(self, X, y=None, is_train=True):
        if is_train:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            kf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

            # Store CV scores
            self.cv_scores = []

            for i, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_valid_fold = X.iloc[valid_idx]
                y_valid_fold = y.iloc[valid_idx]

                fold_preds = np.zeros(len(valid_idx))
                for j, (name, model) in enumerate(self.base_models.items()):
                    model.fit(X_train_fold, y_train_fold)
                    proba = model.predict_proba(X_valid_fold)[:, 1]
                    meta_features[valid_idx, j] = proba
                    fold_preds += proba / len(self.base_models)

                # Calculate and store fold score
                fold_score = roc_auc_score(y_valid_fold, fold_preds)
                self.cv_scores.append(fold_score)
                print(f"Fold {i+1} AUC: {fold_score:.4f}")

            print(f"\nMean CV AUC: {np.mean(self.cv_scores):.4f} (+/- {np.std(self.cv_scores):.4f})")
            return meta_features
        else:
            meta_features = np.zeros((X.shape[0], len(self.base_models)))
            for j, (name, model) in enumerate(self.base_models.items()):
                meta_features[:, j] = model.predict_proba(X)[:, 1]
            return meta_features

    def fit(self, X, y):
        print("Generating meta-features and CV scores...")
        meta_features = self.generate_meta_features(X, y, is_train=True)
        self.meta_model.fit(meta_features, y)

        print("\nTraining base models on full data...")
        for name, model in self.base_models.items():
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        meta_features = self.generate_meta_features(X, is_train=False)
        return self.meta_model.predict_proba(meta_features)[:, 1]

    def evaluate(self, X, y, threshold=0.5):
        """Evaluate model performance with multiple metrics"""
        y_pred_proba = self.predict_proba(X)
        y_pred = (y_pred_proba >= threshold).astype(int)

        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()

        metrics = {
            'accuracy': accuracy_score(y, y_pred),
            'auc_score': roc_auc_score(y, y_pred_proba),
            'avg_precision': average_precision_score(y, y_pred_proba),
            'confusion_matrix': {
                'tn': tn,
                'fp': fp,
                'fn': fn,
                'tp': tp
            }
        }

        print("\nModel Performance Metrics:")
        print("=" * 50)
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"AUC Score: {metrics['auc_score']:.4f}")
        print(f"Average Precision: {metrics['avg_precision']:.4f}")

        print("\nConfusion Matrix:")
        print("-" * 50)
        print(f"True Negatives: {tn}")
        print(f"False Positives: {fp}")
        print(f"False Negatives: {fn}")
        print(f"True Positives: {tp}")

        # Calculate and print additional metrics
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        print("\nAdditional Metrics:")
        print("-" * 50)
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

        return metrics


# Load development data
print("Loading development data...")
dev_data = pd.read_csv('/content/cleaned_dev.csv')

# Prepare features and target
target = 'bad_flag'
features = [col for col in dev_data.columns if col not in [target, 'account_number']]

# Split development data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    dev_data[features],
    dev_data[target],
    test_size=0.2,
    random_state=42,
    stratify=dev_data[target]  # Ensure balanced split
)

print(f"\nData split:")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")
print(f"Positive class distribution in train: {(y_train==1).mean():.4f}")
print(f"Positive class distribution in test: {(y_test==1).mean():.4f}")

# Initialize and train model
print("\nTraining model...")
model = CreditScorer()
model.fit(X_train, y_train)

# Evaluate on test set
print("\nEvaluating on test set...")
test_metrics = model.evaluate(X_test, y_test)

# Create temporary directory for validation data
temp_dir = 'temp_extracted'
os.makedirs(temp_dir, exist_ok=True)

try:
    # Extract and load validation data
    print("\nLoading validation data...")
    val_csv_path = extract_zip('/content/validation_data_to_be_shared 3.zip', temp_dir)
    val_data = pd.read_csv(val_csv_path)

    print("\nGenerating predictions for validation data...")
    # Now retrain on full development data before making validation predictions
    print("\nRetraining model on full development data...")
    model = CreditScorer()
    model.fit(dev_data[features], dev_data[target])

    # Generate predictions for validation data
    val_predictions = model.predict_proba(val_data[features])

    # Create submission file
    submission = pd.DataFrame({
        'account_number': val_data['account_number'],
        'predicted_probability': val_predictions
    })

    print("\nSaving predictions...")
    submission.to_csv('predictions2.csv', index=False)
    print("\nDone! Predictions saved to 'predictions.csv'")

finally:
    # Clean up temporary files
    import shutil
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)